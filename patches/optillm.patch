diff --git a/optillm/bon.py b/optillm/bon.py
index e22ee18..1f417ab 100644
--- a/optillm/bon.py
+++ b/optillm/bon.py
@@ -93,13 +93,22 @@ def best_of_n_sampling(system_prompt: str, initial_query: str, client, model: st
         return "Error: Could not generate any completions", bon_completion_tokens
 
     # Rate the completions
-    rating_messages = messages.copy()
-    rating_messages.append({"role": "system", "content": "Rate the following responses on a scale from 0 to 10, where 0 is poor and 10 is excellent. Consider factors such as relevance, coherence, and helpfulness. Respond with only a number."})
+    rating_instruction = (
+        "Rate the following responses on a scale from 0 to 10, where 0 is poor "
+        "and 10 is excellent. Consider factors such as relevance, coherence, and "
+        "helpfulness. Respond with only a number."
+    )
+    base_system = (system_prompt or "").strip()
+    rating_system = f"{base_system}\n\n{rating_instruction}".strip() if base_system else rating_instruction
 
     ratings = []
     for completion in completions:
-        rating_messages.append({"role": "assistant", "content": completion})
-        rating_messages.append({"role": "user", "content": "Rate the above response:"})
+        rating_messages = [
+            {"role": "system", "content": rating_system},
+            {"role": "user", "content": initial_query},
+            {"role": "assistant", "content": completion},
+            {"role": "user", "content": "Rate the above response:"}
+        ]
         
         provider_request = {
             "model": model,
diff --git a/optillm/server.py b/optillm/server.py
index 243c73c..f404a36 100644
--- a/optillm/server.py
+++ b/optillm/server.py
@@ -370,6 +370,11 @@ def parse_combined_approach(model: str, known_approaches: list, plugin_approache
 
     for part in parts:
         if parsing_approaches:
+            if part == "router":
+                approaches.append(part)
+                model_parts.append(part)
+                parsing_approaches = False
+                continue
             if part in known_approaches or part in plugin_approaches:
                 approaches.append(part)
             elif '&' in part:
@@ -719,7 +724,7 @@ def proxy():
     max_tokens = data.get('max_tokens')
 
     # Explicit keys that we are already handling
-    explicit_keys = {'stream', 'messages', 'model', 'n', 'response_format', 'max_completion_tokens', 'max_tokens'}
+    explicit_keys = {'stream', 'messages', 'model', 'n', 'response_format', 'max_completion_tokens', 'max_tokens', 'optillm_base_model'}
 
     # Copy the rest into request_config
     request_config = {k: v for k, v in data.items() if k not in explicit_keys}
@@ -741,6 +746,15 @@ def proxy():
     elif max_tokens is not None:
         request_config['max_tokens'] = max_tokens
 
+    optillm_base_model = data.get('optillm_base_model')
+    if not optillm_base_model:
+        metadata = data.get('metadata')
+        if isinstance(metadata, dict):
+            optillm_base_model = metadata.get('base_model')
+
+    if optillm_base_model:
+        model = optillm_base_model
+
     optillm_approach = data.get('optillm_approach', server_config['approach'])
     logger.debug(data)
     server_config['mcts_depth'] = data.get('mcts_depth', server_config['mcts_depth'])
@@ -752,6 +766,10 @@ def proxy():
     if message_optillm_approach:
         optillm_approach = message_optillm_approach
 
+    # Do not forward optillm_approach / optillm_base_model to upstream providers
+    request_config.pop('optillm_approach', None)
+    request_config.pop('optillm_base_model', None)
+
     if optillm_approach != "auto":
         model = f"{optillm_approach}-{model}"
 
@@ -991,7 +1009,7 @@ def parse_args():
     # Define arguments and their corresponding environment variables
     args_env = [
         ("--optillm-api-key", "OPTILLM_API_KEY", str, "", "Optional API key for client authentication to optillm"),
-        ("--approach", "OPTILLM_APPROACH", str, "auto", "Inference approach to use", known_approaches + list(plugin_approaches.keys())),
+        ("--approach", "OPTILLM_APPROACH", str, "auto", "Inference approach to use", sorted(set(known_approaches + list(plugin_approaches.keys()) + ["auto"]))),
        ("--mcts-simulations", "OPTILLM_SIMULATIONS", int, 2, "Number of MCTS simulations"),
        ("--mcts-exploration", "OPTILLM_EXPLORATION", float, 0.2, "Exploration weight for MCTS"),
        ("--mcts-depth", "OPTILLM_DEPTH", int, 1, "Simulation depth for MCTS"),

diff --git a/optillm/server.py b/optillm/server.py
index 243c73c..c6b0c98 100644
--- a/optillm/server.py
+++ b/optillm/server.py
@@ -1,5 +1,6 @@
 import argparse
 import logging
+import math
 import os
 import secrets
 import time
@@
 def count_reasoning_tokens(text: str, tokenizer=None) -> int:
@@
     content_length = len(thinking_content.strip())
     return max(1, content_length // 4) if content_length > 0 else 0
+
+_prompt_tokenizer = None
+
+def _get_prompt_tokenizer():
+    global _prompt_tokenizer
+    if _prompt_tokenizer is not None:
+        return _prompt_tokenizer
+    try:
+        import tiktoken
+        _prompt_tokenizer = tiktoken.get_encoding("cl100k_base")
+    except Exception as e:
+        logger.info(f"tiktoken unavailable for prompt token counting: {e}")
+        _prompt_tokenizer = None
+    return _prompt_tokenizer
+
+def _rough_token_count(text: str) -> int:
+    if not text:
+        return 0
+    return max(1, math.ceil(len(text) / 4))
+
+def count_prompt_tokens(messages, tokenizer=None) -> int:
+    if not messages or not isinstance(messages, list):
+        return 0
+    parts = []
+    for msg in messages:
+        if not isinstance(msg, dict):
+            continue
+        role = msg.get("role", "")
+        content = msg.get("content", "")
+        if isinstance(content, (dict, list)):
+            try:
+                content = json.dumps(content, ensure_ascii=False)
+            except Exception:
+                content = str(content)
+        parts.append(f"{role}: {content}")
+    text = "\n".join(parts).strip()
+    if not text:
+        return 0
+    tok = tokenizer or _get_prompt_tokenizer()
+    if tok and hasattr(tok, "encode"):
+        try:
+            return len(tok.encode(text))
+        except Exception as e:
+            logger.warning(f"Failed to count prompt tokens with tokenizer: {e}")
+    return _rough_token_count(text)
@@
-        response_data = {
+        prompt_tokens = count_prompt_tokens(data.get('messages', []))
+        response_data = {
             'model': model,
             'choices': [],
             'usage': {
-                'completion_tokens': completion_tokens,
+                'prompt_tokens': prompt_tokens,
+                'completion_tokens': completion_tokens,
+                'total_tokens': prompt_tokens + completion_tokens,
                 'completion_tokens_details': {
                     'reasoning_tokens': reasoning_tokens
                 }
             }
         }
@@
-                    response_dict = {
+                    prompt_tokens = count_prompt_tokens(data.get('messages', []))
+                    response_dict = {
                         "id": f"chatcmpl-{int(time.time()*1000)}-{i}",
                         "object": "chat.completion",
                         "created": int(time.time()),
                         "model": model,
                         "choices": choices,
                         "usage": {
-                            "prompt_tokens": 0,  # Will be calculated properly later
+                            "prompt_tokens": prompt_tokens,
                             "completion_tokens": completion_tokens if isinstance(completion_tokens, int) else 0,
-                            "total_tokens": completion_tokens if isinstance(completion_tokens, int) else 0
+                            "total_tokens": (prompt_tokens + completion_tokens) if isinstance(completion_tokens, int) else prompt_tokens
                         }
                     }
